\chapter{Background Estimate} \label{chapter:background}

\section{Data-Driven Background Estimate}

    Selection does not remove all non-signal events.
    Indeed, most of the events remaining in the data are very likely background.
    Since these background events cannot be effectively differentiated from signal data,
        a technique must be used to estimate how much of the observed data is coming from background.
    Specifically, the analysis employs a method to predict the kinematic shape (in \mhh)
        of the background events within the 4b Signal Region,
        without directly observing that set of events.

    For the \vbfproc process, the background consists almost entirely of QCD jet events.
    The usual approach of simulating the background using Monte-Carlo techniques is not feasible,
        as such simulations are inaccurate when trying to model such complex, many-jet processes. [TODO: is this the actual reason?]
    Instead, the yield from background in the SR data is predicted based on the background yield in different subsets of the data.
        
    The fundamental assumption for this data-driven background estimate
        is that background events with only two b-tagged jets (2b data)
        will have kinematics very similar to the background events with 4 b-tagged (4b) jets.
    Thus, the kinematic distribution of the 2b signal region can be used as an estimate of the background in the 4b signal region.
    Unfortunately, the kinematics of the 2b and 4b data are not expected to match completely,
        so the 2b signal data must undergo a "reweighting" process,
        which adjusts the kinematic distribution to match that of 4b.
    This reweighting takes the form of a function $R$,
        which for each event $i$ takes a series of kinematic variables $x_i$ as an argument,
        and returns a reweighting value $r_i$ which scales that event's contribution to the kinematic distributions:
        $r_i = R(x_i)$.
    Deriving this function, and determining the appropriate inputs for it,
        are the challenges addressed in the following sections.

\section{Neural Network Training and Uncertainty} \label{sec:nn_training}

    The reweighting function is derived using machine learning techniques.
    A neural network (NN) is trained to identify how to reweight 2b data such that the cumalitive kinematic distribution looks like that of 4b data.
    In order to improve stability in the reweighting function,
        an ensemble of 100 networks is trained, and the median of their calculations is used (called the ``Nominal Estimate'').
    Each neural network instance $j$ produces a reweighting factor $w_{ij}$, and has a normalization factor $\alpha_j$ associated with it.
    This normalization factor is calculated so the total yield of all 2b reweighted events matches that of the 4b yield in the same region. 
    % TODO: I need to emphasize that the normalization is set to match the yield of
    % the region the NN was *trained* in. 
    % So in going from 2b CR to 4b CR, there is some scaling factor "a"
    % That same "a" is then applied when going from 2b SR to 4b SR
    % Key point being that we are not artificially forcing the 
    % 4b SR background yield to match the 4b SR data yield.
    So for $N$ total 2b events and $N'$ total 4b events:
        \begin{equation}
        \sum_{i=1}^{N} \alpha_j w_{ij} = N'
        \end{equation}

    The nominal estimate $\tilde{w}$ constructed from the median of these networks will not necessarily satisfy this same relation,
        so it is given its own separate normalization $\tilde \alpha$:
        \begin{equation}
        \sum_{i=1}^{N} \tilde \alpha \tilde w_i = N'
        \end{equation}

    \begin{figure}[!htbp]
        \subfloat[Data before reweighting]{
            \includegraphics[width=0.5\linewidth,height=\textheight,keepaspectratio]{background/NR-MAY21-crypto-m_hh-control-no_rw-allyr-3b1f-dEtahhcat_inclusive}
        }
        \subfloat[Data after reweighting]{
            \includegraphics[width=0.5\linewidth,height=\textheight,keepaspectratio]{background/NR-MAY21-crypto-m_hh-control-NN-allyr-3b1f-dEtahhcat_inclusive}
        }
        \caption{
            The \mhh kinematic distribution of the 2b VS 4b regions, before and after reweighting the 2b region.
            [TODO: update these plots and ditch the 3b1f stuff for 4b CR1]
        }
        \label{fig:data_mhh_reweight}
    \end{figure}

    The problem with this training approach, is that the 4b signal region cannot be used in the training.
    Instead, the neural networks are trained using the CR1 data,
        and are optimized to reweight the 2b CR data to resemble the 4b CR data.
    Because the reweighting function is trained on a different kinematic region than the signal region,
        a systematic uncertainty is associated with it.
    This is calculated by training a different ensemble of NNs on Control Region 2
    % TODO insert picture of signal quad regions
    As seen in Figure TODO, the signal region is split into four quadrants,
        with the divisions angled to match those of CR1 and CR2
    The nominal reweighting is performed using the entire signal region,
        with the events binned by their \mhh value to form a nominal histogram.
    Four alternate histograms are then generated from events reweighted using
        \textit{both} the CR1-trained NN and the CR2-trained NN.
    Specifically, for each of these alternate histrograms,
        events from three out of the four signal region quadrants will be reweighted using the CR1 NN.
    The events from the fourth quadrant are reweighted via the CR2 NN.
    A bin-by-bin (absolute) difference between the nominal histogram and each of the alternate histograms 
        is then used to produce the four systematic ``shape'' uncertainties.

    In addition to the systematic shape uncertainty,
        an additional statistical ``bootstrap'' uncertainty is calculated to account for the variation of the bootstrap neural networks.
    This uncertainty is calculated by producing an \mhh histogram for each of the 100 bootstrap NNs.
    The standard deviation of each bin across all 100 bootstrap histograms is calculated,
        and then assigned as a statistical error for that bin of the nominal histogram.


\section{VBF Background Reweighting Variables} \label{sec:vbf_bgdNNRW}

    The Neural Network used to reweight the 2b-Tagged CR1 data into the 4b Signal region
        uses a different set of variables when trained for the VBF process than it does for ggF.
    This list of variables was selected through two correlation tests,
        the first of which was determining how strongly correlated different variables are to \mhh
        (Figure \ref{fig:mhh_corr}).

    \begin{figure}[!htbp]
        \subfloat[$\deta \leq 1.5$]{
            \includegraphics[width=0.5\linewidth,height=\textheight,keepaspectratio]{background/correlation_control2b_detahh-LTE1p5_hist_m_hh.pdf}
        }
        \subfloat[$\deta > 1.5$]{
            \includegraphics[width=0.5\linewidth,height=\textheight,keepaspectratio]{background/correlation_control2b_detahh-GT1p5_hist_m_hh.pdf}
        }
        \caption{
            The Pearson Correlation Coefficients associated with \mhh for the 2b CR1 data.
            The regions in which $\deta \leq 1.5$ and $\deta>1.5$ are separately displayed.
            The higher up on the chart a variable is, the more strongly correlated it is to \mhh,
                with \mhh itself at the top with a coefficient of 1.
            Note that most of the VBF-specific variables, such as vbf\_mjj, are very poorly correlated to \mhh and thus not proffered.
        }
        \label{fig:mhh_corr}
    \end{figure}

    Based on their high correlation to \mhh in both \deta regions, 23 variables were then selected for further study.
    These 23 variables were plotted against each other in a correlation matrix,
        which displayed how strongly correlated these variables are to each other.
    A final set of seven variables was selected based (Table \ref{tab:vbf_NNRW_vars}) on which variables were \textit{least} correlated to each other,
        in order to avoid variables carrying redundant information (Figure \ref{fig:vbf_corr_matrix}).

    \begin{figure}[!htbp]
        \subfloat[$\deta \leq 1.5$]{
            \includegraphics[width=0.5\linewidth,height=\textheight,keepaspectratio]{background/correlation_control2b_detahh-LTE1p5_matrix.pdf}
        }
        \subfloat[$\deta > 1.5$]{
            \includegraphics[width=0.5\linewidth,height=\textheight,keepaspectratio]{background/correlation_control2b_detahh-GT1p5_matrix.pdf}
        }
        \caption{
            The Pearson Correlation Coefficients matrix associated with variables in the 2b Control Region data.
            The regions in which $\deta \leq 1.5$ and $\deta>1.5$ are separately displayed.
            Bins with more saturated (i.e.\ more blue or more red) colors indicate strong correlation between those two variables.
            All variables are of course 100\% correlated to themselves, hence the deep red line running down the diagonal.
            The most highly preferred variables are those with correlation coefficients near 0 for as many other variables as possible.
            If a variable (i.e.\ m\_max\_dj) is selected, any other variables with strong correlations to it (i.e.\ pT\_h2)
                are then strongly disfavored for later selection.
        }
        \label{fig:vbf_corr_matrix}
    \end{figure}


    \begin{table}[!htbp] \centering \footnotesize
    \caption{Final Set of Neural Network Variables}
    \label{tab:vbf_NNRW_vars}
    \begin{tabular}{ |l|l|l| }
        \hline
        \textbf {Variable} & \textbf {Internal Name} & \textbf {Description} \\
        \hline
        $M_{max \Delta j}$ & \code{m\_max\_dj}         & 
            Take the di-jet mass of the six possible pairings\\
            && of the four higgsâ€™ candidate jets;\\
            && this is the maximum di-jet mass of those pairings \\ 
        \hline
        $M_{min \Delta j}$ & \code{m\_min\_dj}         & 
            As above, but the minimum \\
        \hline
        $E_{H1}$           & \code{E\_h1}              & 
            Energy of the leading-$p_T$ reconstructed Higgs \\
        \hline
        $E_{H2}$           & \code{E\_h2}              & 
            Energy of the sub-leading-$p_T$ reconstructed Higgs \\
        \hline
        Xwt-tag            & \code{X\_wt\_tag}         & 
            $\log\left(X_{Wt}\right)$, where $X_{Wt}$ is the variable used for the top veto \\
        \hline
        $\eta_i$           & \code{eta\_i}             & 
            Average $\eta$ of the four Higgs decay jets \\
        \hline
        Pairing Score 2    & \code{pairing\_score\_2 } & 
            This changes depending on the Higgs pairing algorithm used. \\
            &&The current algorithm used is minDR. \\
            &&With four jets, there are three possible pairings of the jets. \\
            &&The pairings are ranked by the $\Delta_R$ of the leading-$p_T$ Higgs candidate. \\
            &&Pairing Score 2 is the $\Delta-R$ of the second-smallest of these pairings. \\
        \hline
    \end{tabular} \end{table}
