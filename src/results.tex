% Need to discuss how we put data, the background estimate, and the signal model together
%     and make a claim as to the compatibility of the hypothesis with the data.
% Largely, this means I need to finally understand how pyhf actually works and what the hell the limit framework is doing.
\chapter{Results} \label{chapter:results}

%
%
%
%    I'm not sure I actually have to explain Baye's theorem here...
%    it doesn't appear to be obviously used, or if it is, we're implicitly using the "Uniform" Prior
%    Ask Steve
%
%    L gives probability of seeing the data we have, based on the given model.
%    We need the probability that the given model is the one responsible for the data we see.
%    i.e. we have P(data|model), but we need P(model|data)
%
%    To do this, we need Baye's rule, which comes from the basic 'anding' of probabilities:
%    P(a \& b) = P(a)*P(b|a) , or P(a \& b) = P(b)*P(a|b) . Thus
%    P(a)*P(b|a) = P(b)*P(a|b) 
%    P(b|a) = P(a|b) P(b) / P(a)
%    So we need P(model|data) = P(data|model) * P(model) / P(data)
%
%    For our data we have to use the extended L, which accounts for poisson stats.
%    Basic test is to test mu*S+B for what value of mu is compatible with data
%
%        
%
%

% NOTE: From here on, try to keep the p-value concept,
%   as well as the PDF and C-PDF, as a central focus.
% It's easy to conceptualize what a p-value is,
% so you should keep returning to it in order to retain 
% a concrete basis for all the weird math you're about to do
\section{Statistical Mathematics}

    I have thus far established how the analysis has collected its data,
        how it has estimated the amount of background present in that data,
        and the hypothesis for the HH process to be tested with that data.
    With these assembled, the final step is to arrange them together in order to make a definitive statement
        as to the validity or incompatibility of the hypothesis with the provided data.
    More plainly, was the di-Higgs process detected in the data,
        and what were the values of the $\kappa$ couplings involved in its production.

    %Statistics is a powerful tool in science,
    %    but one which can be very misleading if not used carefully.
    %%The oft-used quote comparing lies and statistics exists for a reason;
    %%    both can lead to incorrect scientific conclusions.
    %%But whereas a lie can be caught by a simple slip of the tongue,
    %%    it can take scientists years to discover a slight (intentional or not) mishandling of statistics.
    %Unfortunately, the use of statistical methods is not optional.
    Need some transition here...
    In this section, I want to explain why statistics are required
        both in general and for this analysis specifically.
    Moreover, I want to discuss the specific statistical techniques this analysis uses,
        and conclude with what those techniques reveal in light of the data presented.

    %To begin, let me propose a much simpler experiment than the one described in this analysis.
    %I have a coin, which I suspect may be biased to one side.
    %How can I test this?
    %A fair coin has a 50/50 chance of landing on either side.
    %This means that, were I to flip the coin a large number of times,
    %    I would expect a roughly equal number of heads and tails.
    %Significant deviation from this ratio (e.g.\ 3 million heads to 1 million tails)
    %    would indicate an obvious bias in the coin.
    %The conclusion becomes far less obvious however, if the coin is flipped only a few times.
    %Even if every flip comes up as heads, no meaningful conclusion can be made if the coin was only flipped e.g.\ four times.
    %The crucial questions raised here, which statistical methods are able to address,
    %    are how much data is needed to make a decisive statement about a hypothesis,
    %    and what kind of statements can be made in lieu of that data.

    %The first step to testing a hypothesis,
    %    is to know precisely how \textit{likely} any particular outcome is based on that hypothesis.
    %The hypothesis for the coin flip is that the probability $p$ of any given flip being heads is 50\%.
    %For an experiment consisting of a number of flips $N$,
    %    the overall probability of seeing an amount of heads $n$ is given by the binomial distribution:
    %\begin{equation}
    %    P(n|p) = \tinymatrix{N\\n} p^n (1-p)^{N-n}
    %\end{equation}
    %Where $P(n|p)$ is read as ``the probability of seeing $n$ heads \textit{given} their probability $p$.''
    %With this, 
    %
    %basic binomial distribution (coin flip) allows obvious p-test. 
    %Using a concrete toy example with numbers,
    %Show a binomial PDF distro for N flips, and where on that distro the "data" lies
    %Show a C-PDF, and again where the data is,
    %    and explain that the "unlikeliness" can be used as a test metric, called a "p-value"
    %Show of whether or not theory is compatible with data.

    % Introduce poisson statistics with single counting variable and only signal w/ toy example.
    % Discuss difference between coin toss and radioactivity.
    % i.e. that with a coin toss, the number to tosses is entirely determined by me, and is thus fixed.
    % For decay and such, the number of events is completely random, and what is fixed is how much time (luminosity)
    %     the experiment is given.
    Like most particle physics processes [all?], the $VBF \to HH \to 4b$ process is poisonian in nature.
    The number $n$ of diHiggs process that are observed in ATLAS can thus be modeled with a poisson distribution:
    \begin{equation}
        P(n|\nu) = \frac{ \nu^n e^{-\nu} }{n!}
    \end{equation}
    Where $\nu$ is the expected event yield for the process.
    The compatibility of a hypothesis with data can be established by use of a p-value test.
    For example, take the case of a simple alpha emitter,
        which I hypothesize emits radiation at a rate of once per minute.
    After one hour, I would expect on average 60 events.
    running the experiment, I find that 80 events were detected.
    Plotting the poisson distribution for $P(n|60)$ (Figure \ref{fig:poisson_toy_sig:pdf}),
        I can see how likely an observation of 80 events is.

    \begin{figure} %TODO
        \centering
        \begin{subfigure}{0.4\textwidth} 
            \input{results/mu_pdf} 
            \caption{Toy Poisson PDF}
            \label{fig:poisson_toy_sig:pdf}
        \end{subfigure}
        \begin{subfigure}{0.4\textwidth}
            \input{results/mu_pdf} 
            \caption{Toy Poisson Cumalitive PDF}
            \label{fig:poisson_toy_sig:Cpdf}
        \end{subfigure}
        \caption{
            A plot of the probability distribution function (PDF)
                and cumulative PDF for a toy Poisson experiment.
        }
    \end{figure}

    The key tool that will be used throughout the rest of this analysis is a ``p-test'',
        which is to find the probability that, were my hypothesis correct,
        I would obtain data with a value \textit{at least as extreme} as what I measured\footnote{
            There are actually several different variations of a p-test;
            this is specifically a one-sided p-test}.
    So the p-value for an event rate of 80 given an expected average of 60 can be found by taking:
    \begin{equation}
        \textrm{p-value}(n=80) = P(80|60) + P(81|60) + P(82|60) ... P(\inf|60) = \sum\limits_{m=n}^\inf P(m|60)
    \end{equation}
    This is best visualized using the \textit{cumalitive} probability distribution function (Figure \ref{fig:poisson_toy_sig:Cpdf}).
    In this example, the p-value would be [TODO].
    A typical standard for many statistical tests, and the one used in this analysis,
        is to establish a ``Confidence Level'' (CL) in the results of an experiment at a level of 95\%. 
    The confidence level is simply $1-p$, so a CL of 0.95 corresponds to a p-value of less than 0.05.
    Because this toy experiment has a pvalue much larger than 0.05,
        the conclusion would be that ``the hypothesis was compatible with the data within a CL of 95\%.''
    [Hey Steve, assuming I should even keep my example here, I'm not sure I'm wording this precisely as needed 
        (I know these conclusion statements are touchy with their wording;
        should I adjust the toy numbers to make for a better example?)]


    % Introduce background
    % Ditch toy, pull out actual Background and Signal (SM) event yields.
    % Should also probably dig up the "sensitivity" metric and show how bad that is here as well.
    % CLs = CLs+b/CLb explained in \cite{Barlow:2019svl} (pg. 192)
    % The abysmal performance here should justify splitting the event into bins,
    %     in order to identify regions of Mhh that we are more sensitive to.
    % Using multiple bins however, dramatically complicates the statistical analysis.
    % Enter the profile likelihood fit.
    Moving on from the toy example, the model being tested in this analysis involves a background event rate
        in addition to the signal hypothesis being considered.
    The average number of events expected is then $\nu = S + B$,
        where $S$ and $B$ are the signal and background rate yields respectively.
    The confidence limits in such a situation are handeled in different ways from one analysis to another,
        but in this analysis the pvalue of the signal alone is given by
    \begin{equation}
        P_S = \frac{P_{S+B}}{1 - P_B}
    \end{equation}
    Where $P_{S+B}$ is the pvalue of the hypothesis assuming the signal is present,
        and $P_B$ is the pvalue of the background-only hypothesis
        (often called the ``null hypothesis,'' $H_0$).
    The total yields of the data, Background estimate, and SM signal hypothesis are provided in Table \ref{tab:event_yield},
        and their PDF and cumulative PDF can be seen in
        Figures \ref{fig:poisson_sig:pdf} and \ref{fig:poisson_sig:Cpdf}.

    \begin{table}[tbh]
       \begin{center}
           \caption{Estimated and Observed Event Yields [FIXME: put in the real numbers]}
           \label{tab:event_yield}
           \footnotesize
           \begin{tabular}{|l|l|}
           \toprule
               Type  &	Event Yield \\
               \midrule
               Background Estimate  & 300000 \\
               Signal Hypothesis (SM) & 1 \\
               %Signal Hypothesis (\kvv=3) & 400 \\
               Observed Data & 300000 \\
           \bottomrule
           \end{tabular}
       \end{center}
    \end{table}

    \begin{figure} %TODO
        \centering
        \begin{subfigure}{0.4\textwidth} 
            \input{results/mu_pdf} 
            \caption{Poisson PDF}
            \label{fig:poisson_sig:pdf}
        \end{subfigure}
        \begin{subfigure}{0.4\textwidth}
            \input{results/mu_pdf} 
            \caption{Poisson Cumalitive PDF}
            \label{fig:poisson_sig:Cpdf}
        \end{subfigure}
        \caption{
            A plot of the probability distribution function (PDF)
                and cumulative PDF for the estimated background and simulated signal,
                with the observed yield denoted in both.
        }
    \end{figure}

    The pvalue of the signal based on the observed yield is .001 [FIXME],
        which means the CL is over 99\%.
    This indicates that that the signal is very compatible with the data,
        but by no means indicates proof of the diHiggs process.
    To understand why, one need only look at the CL on the background-only hypothesis.
    Proof of the diHiggs process would necessitate an excess of events so significant that
        the null hypothesis must be rejected (typically $P_B < 10^{-7}$, the ``$5\sigma$'' discovery limit).
    Here though, $P_B \approx 0.5$, which is far from being able to reject the null hypothesis.

    The issue of course, is that the signal is utterly dwarfed by the quantity of background events.
    A method of circumventing this scale issue
        is to break the event count up across a number of different
        bins and categories dependent on kinematic observables of the events.
    The signal and background events are expected to have their events distributed differently across such observables.
    Thus, even though there are vastly more background events overall,
        it is entirely possible for there to exist a region in the observable space
        in which there are a comparable number of signal events.
    In this analysis, the primary observable variable is the diHiggs invariant mass, \mhh.
    Ultimately, the final test of the hypothesis will still be to create a cumulative PDF for the distribution,
        obtain the value of the cumulative PDF for the observed data, and use that to derive a CL for the signal.
    With multiple bins (as well as categries and uncertainties), the deriviation of the cumulative PDF becomes much more complex.
    The following section will describe precisely how all these aspects of analysis are combined together,
        producing a much more powerful statistical analysis.

    % You should take your sig/bgd mhh plot and turn the ratio bit into a sensitivity plot, of Si/sqrt(Si+Bi)
    % Maybe include it here? TODO?
    % Maybe do it for k2v=3 as well, since I think that might be useless in total yield, but powerful when binned,
    % which helps justify all this work


\FloatBarrier
% Discuss sources of error, assumptions, categorization, concept of mu values;
% basically all the complicated stuff the limit framework is doing
% Make the final goal to be establishing a p-value via the cumulative PDF distro,
%   as was done for the simple coin toss example (to bring things full circle)
\section{VBF \to HH \to 4b Limit-Setting Framework}
    
    %What is a test statistic and why do we need it?
    The statistical technique used in this analysis to obtain a cumalitive PDF of the signal and background models
        is based on the use of a ''test statistic'' derived through a \textit{profile likelihood fit}.
    Test statistics come in many forms, but the one used here is known as the \qtil test statistic.
    \qtil is technically defined in a single equation,
        but that equation involves enough parameters that it is worth breaking it down into steps first.

    % Introduce mu*S+B format.
    % Discuss formula for L specifically in this analysis
    %    (emphasis on explaining the bits in parentheses):

    %    L = product[ for each category (2: eta hi and lo) 
    %        product[for each bin: poissons]
    %        * product[ nuissance params (4 for bgd shape error, 1 for norm error) ] 
    %    ]
    %    
    The first step to deriving \qtil, is to slightly redefine the expectation value for the number of events 
        with a scaling factor $\mu$ applied to the signal yield.
    $\mu$ is reffered to as the ``parameter of interest'' (PoI),
        and allows the yield of the signal process to be adjusted to some ideal value that best fits the data.
    As well, there are now multiple bins, each with their own expectation values,
        so the new definition of the expectation value is
    \begin{equation}
        \nu_i(\mu) = \mu S_i + B_i
    \end{equation}
    Instead of a probability distribution function, now a ``likelihood'' function $L$ will be used,
        which is the joint probability of the observed data $n$ in each of the bins
        (which are in turn all poisson distributed with expectation value $\nu_i$)
    \begin{equation}
        L(n,\mu) = \product \limits_{i=1}^{N} P(n_i | \nu_i)
            = \product \limits_{i=1}^{N} \frac{ (\mu S_i + B_i)^{n_i} e^{\mu S_i + B_i} }{n_i!}
    \end{equation}
    Where $i$ runs over all the $N$ bins.
    Note that unlike a PDF, the sum of all outcomes from the likelihood function will \textit{not} sum to unity,
        and so the likelihood function does not constitute a probability.

    % Discuss uncertainties and categories
    Uncertainties present in the background estimate add another layer of complexity to the likelihood function,
        because uncertain parameters must be fit as well as the parameter of interest.
    The expectation value is further adjusted to be $\nu_i(\mu, \Theta) = \mu S + \Theta B$,
        where $\Theta$ is a product of five scaling values 
    \begin{equation}
        \Theta = \product \limits_{a=0}^{4}  \theta^a
    \end{equation}
    These scaling values, called ``nuissance parameters,''
        correspond to the four shape uncertainties $\sigma_i^a, a\element{1,2,3,4}$
        and one normalization uncertainty $\sigma^0$ ($\sigma^0$ does not vary by bin),
        which were described in Section \ref{sec:nn_training}
        [\TODO: you need to go back and describe these uncertainties better now that you understand them].
    In this analysis, the background uncertainty is modeled as a Gaussian $\script{G}$
        with expectation value $\theta^a B_i$ and standard deviation $\sigma_i^a$.
    Extending the likelihood function produces
    \begin{equation} \begin{split}
        L(n,\mu,\Theta) &= \product \limits_{i=1}^{N} P_\textrm{poiss}(n_i | \nu_i) 
             \product \limits_{i=1}^{N} P_\textrm{gauss}(n_i | \nu_i, \sigma_i^a) 
        \\&= \product \limits_{i=1}^{N} \frac{ (\mu S_i + \Theta B_i)^{n_i} e^{\mu S_i + \Theta B_i} }{n_i!} \times
            \product \limits_{a=0}^5 \product \limits_{i=1}^{N} \frac{1}{\sigma_i^a \sqrt{2\pi}} e^{
                -\frac{1}{2}\left(\frac{n_i- (\mu S_i + \Theta B_i)}{\sigma_i^a}\right)^2
    \end{split} \end{equation}

    Finally, in order to increase sensitivity further, events are split into two different categories
        based on whether the \deta seperation between the reconstructed Higgs bosons is greater or less than 1.5.
    The full likelihood function is then a product over the two categories
    \begin{equation}
        L(n,\mu,\Theta) &= \product \limits_{j=1}^{2}
             \product \limits_{i=1}^{N} P_\textrm{poiss}(n_{ij} | \nu_{ij}) 
             \product \limits_{i=1}^{N} P_\textrm{gauss}(n_{ij} | \nu_{ij}, \sigma_{i}^a) 
    \end{equation}

    % Discuss nuissance parameter fitting
    There is only one parameter of interest ($\mu$), but there are now six parameters $L$ is a function of.
    To address this without overfitting to the data, a maximized fit is performed across the nuissance parameters.
    Such a fit eliminates their presence from the likelihood function,
        but it should be noted that it does so at the cost of significantly extending the final cumulative PDF,
        and thus reducing the overall confidence intervals.
    To proceed, the value of $\mu$ is fixed to the desired scale (for now $\mu=1$),
        and then $L(n,\mu,Theta)$ is maximized over the various values of the $\theta^a$.
    The nuissance parameter values are then fixed to this maximal value $\hat\hat\theta^a$, such that
    \begin{equation}
        \product \limits_{a=0}^{4} \frac{\partial}{\partial \theta^a} L(n,\mu,\Theta) |_{\Theta=\hat\hat\Theta} = 0
    \end{equation}
    Resulting in a fitted $L = L(n,\mu,\hat\hat\Theta)$.
        
    %lambda = L(mu, theta vary-opt) / L(mu-opt, theta-opt);
    As a final fitting step, the likelihood $L$ is normalized by a fully maximized value $\hat L$,
        to form a quantity $\lambda = L / \hat L$.
    $\hat L$ is formed in a similar manner to $L$, but whereas $L$ has a fixed, given value for $\mu$,
        $\hat L$ allows $\mu$ to also be a free parameter.
    It is then maximized with regard to both $\mu$ and the nuissance parameters such that
    \begin{equation}
        \product \limits_{a=0}^{4} \frac{\partial}{\partial \theta^a} \frac{\partial}{\partial \mu} L(n,\mu,\Theta) |_{\Theta=\hat \Theta} |_{\mu=\hat \mu} = 0
    \end{equation}
    A test statistic $t$ can now be constructed with $\lambda$, as
    \begin{equation}
        t = -2 \ln{\lambda} = -2 \ln{\frac{L(\mu, \hat \hat \Theta)}{L(\hat \mu, \hat \Theta)}}
    \end{equation}
    It is worth briefly reviewing what this formula and its pieces mean before moving onto the last step.
    The likelihood $L$ is just the joint probability that every bin of the observed data would have the values they do,
        based on the modeled expectation values.
    Due to the large number of bins, $L$ will always be an extremely small number between 0 and 1.
    By construction, $\hat L$ will always be a joint probabiliy at least as large as $L$, though still between 0 and 1.
    Their ratio $\lambda$ represents how close $L$ is to being maximally likely for the given set of observed data and models.
    $\lambda$ can at most be equal to 1, indicating that $L$ is nearly equal to $\hat L$,
        which in turn indicates that the model fits the observed data as well as it can for any given value of $\mu$.
    In turn, a value near 0 indicates that the observed data is exceedingly unlikely to have come from the model with the given $\mu$,
        compared to the much more likely model based on the optimal value of $\mu$.
    The test statistic $t$ then has a simple interpretation:
        values of $t$ close to 0 indicate high likelihood,
        and the higher the value (up to infinity) the more unlikely it is that the model is compatible with the data.

    %q~ = t with edge cases;
    At last, the desired test statistic \qtil can be derived, as a piecewise variation on $t$ \cite{asymptotic_formulae_for_likelihood}:
    \begin{equation}
        \qtil = \[ \begin{cases}
                -2 \ln{\frac{L(\mu, \hat \hat \Theta)}{L(0, \hat \Theta(\mu=0))}} & \hat \mu < 0,\\
                -2 \ln{\frac{L(\mu, \hat \hat \Theta)}{L(\hat \mu, \hat \Theta)}} & 0 \leq \hat \mu < \leq \mu,\\
                0 & \mu < \hat \mu 
            \end{cases} \]
    \end{equation}
    The middle condition is simply $t$, with the top and bottom edge cases being the defining traits of \qil.
    \qtil in particular is used for this analysis because of these piece-wise conditions,
        which enforce the fact that $\mu$ should never be negative (the signal cannot remove events from the background),
        and that this analysis is only interested in establishing a lower-bound
        [TODO: might need to think about this some more to be sure you know what you're saying here].

    %Single mu vs q~ PDF constructed from monte-carlo distros
    %C-PDF showing where mu-model resides for Psb, Pb, and finally Ps=Psb/(1-Pb)
    %In practice, the q~ CPDF distros are estimated using an asymptotic approximation method\cite{asymptotic_formulae_for_likelihood}.
    %(because the MC method is way too slow)
    Even with the test statistic \qtil available, its use may not be immediately obvious.
    Recall that the goal is to produce a cumalitive probability distribution function,
        which can be compared against to produce a p-value.
    The solution to obtaining a distribution, is to create one using monte-carlo techniques.
    A large number of simulated distributions can be generated, sampled from the signal and background poisson distributions.
    For each of these simulated ``observations'' the test statistic \qtil can be derived.
    The result is a vast number of \qtil values, which together comprise a distribution.
    A cumulative probability distribution can be constructed with this distribution,
        by simply taking the fraction of simulated observations which fall below some value of \qtil.
    Figures \ref{fig:qtil:pdf} and \ref{fig:qtil:Cpdf} demonstrate how this is done,
        as well as where the SM signal model falls with a $\mu$ value of 1.

    \begin{figure} %TODO
        \centering
        \begin{subfigure}{0.4\textwidth} 
            \input{results/mu_pdf} 
            \caption{Values of \qtil for Simulated Distributions}
            \label{fig:qtil:pdf}
        \end{subfigure}
        \begin{subfigure}{0.4\textwidth}
            \input{results/mu_pdf} 
            \caption{Cumalitive PDF of \qtil Test Statistic}
            \label{fig:qtil:Cpdf}
        \end{subfigure}
        \caption{
            Plots of the values and cumalitive PDF based on \qtil,
                with the \qtil value of the observed data highlighted.
            These figures are for demonstration purposes only,
                and conclusions should not be drawn from them.
        }
    \end{figure}

    I would like to note that while I have performed the explicit simulation of the MC distributions for the sake of these figures,
        in practice the MC approach is far too slow.
    Indeed, these figures already took a substantial amount of time to produce, and yet are still quite unreliable and innacurate.
    Many times more distributions would have been needed to produce a reliably stable cumulative PDF.
    What is often done instead, and what this analysis does,
        is to use an asymptotic approximation to the \qtil cumulative distribution function.
    The derivation of this approximation is well beyond the scope of this thesis,
        but is documented extensively by Cowan\cite{asymptotic_formulae_for_likelihood}.
    \qtil itself still needs to be calculated for the observed data,
        but the asymptotic approximation allows for rapid and accurate calculation of its p-value.
    Now that I finally have a robust mechanism for calculating the p-value of a hypothesis,
        the only step left to take is to use it.

\FloatBarrier
% Finally, I need to reveal what our final results actually are, and what they mean.
% This will mostly just be plots of mu values, limit values, and my 2D exclusion plots.
% I can discuss the shapes and stuff here, as well as talk about any mismatches between expected and observed results.
% Be ready to just put in filler results, since we probably won't have unblinded by the time I get here.
\section{Final Interpretation}

SM mu scan plot of signal p-value.
If you do this using one of the other stats (t, or t~ or something),
    I think you can use it to show that pure discovery of the HH process is impossible right now,
    and therefore justify putting limits on the couplings instead.

multi k2v-scan mu plots

Segway from a coupling of mu plots into the idea that, while the SM point is a lost cause,
    other coupling values can be safely ruled out.
Use this to motivate the idea of performing a "coupling scan"
    which can show which values of a coupling can be ruled out.

1D SM k2v plot
1D SM kl plot?

Multi-kl 1D k2v plots

2D k2v/kl plot

Multi-dimensional slice plots
