% Need to discuss how we put data, the background estimate, and the signal model together
%     and make a claim as to the compatibility of the hypothesis with the data.
% Largely, this means I need to finally understand how pyhf actually works and what the hell the limit framework is doing.
\chapter{Results} \label{chapter:results}

%
%
%
%    I'm not sure I actually have to explain Baye's theorem here...
%    it doesn't appear to be obviously used, or if it is, we're implicitly using the "Uniform" Prior
%    Ask Steve
%
%    L gives probability of seeing the data we have, based on the given model.
%    We need the probability that the given model is the one responsible for the data we see.
%    i.e. we have P(data|model), but we need P(model|data)
%
%    To do this, we need Baye's rule, which comes from the basic 'anding' of probabilities:
%    P(a \& b) = P(a)*P(b|a) , or P(a \& b) = P(b)*P(a|b) . Thus
%    P(a)*P(b|a) = P(b)*P(a|b) 
%    P(b|a) = P(a|b) P(b) / P(a)
%    So we need P(model|data) = P(data|model) * P(model) / P(data)
%
%    For our data we have to use the extended L, which accounts for poisson stats.
%    Basic test is to test mu*S+B for what value of mu is compatible with data
%
%        
%
%

\section{Statistical Mathematics}

    I have thus far established how the analysis has collected its data,
        how it has estimated the amount of background present in that data,
        and the hypothesis for the HH process to be tested with that data.
    With these assembled, the final step is to arrange them together in order to make a definitive statement
        as to the validity or incompatibility of the hypothesis with the provided data.
    More plainly, was the di-Higgs process detected in the data,
        and what were the values of the $\kappa$ couplings involved in its production.

    %Statistics is a powerful tool in science,
    %    but one which can be very misleading if not used carefully.
    %%The oft-used quote comparing lies and statistics exists for a reason;
    %%    both can lead to incorrect scientific conclusions.
    %%But whereas a lie can be caught by a simple slip of the tongue,
    %%    it can take scientists years to discover a slight (intentional or not) mishandling of statistics.
    %Unfortunately, the use of statistical methods is not optional.
    Need some transition here...
    In this section, I want to explain why statistics are required
        both in general and for this analysis specifically.
    Moreover, I want to discuss the specific statistical techniques this analysis uses,
        and conclude with what those techniques reveal in light of the data presented.

    To begin, let me propose a much simpler experiment than the one described in this analysis.
    I have a coin, which I suspect may be biased to one side.
    How can I test this?
    A fair coin has a 50/50 chance of landing on either side.
    This means that, were I to flip the coin a large number of times,
        I would expect a roughly equal number of heads and tails.
    Significant deviation from this ratio (e.g.\ 3 million heads to 1 million tails)
        would indicate an obvious bias in the coin.
    The conclusion becomes far less obvious however, if the coin is flipped only a few times.
    Even if every flip comes up as heads, no meaningful conclusion can be made if the coin was only flipped e.g.\ four times.
    The main questions raised here are then:
        how many coin flips are needed to make a decisive statement about the coin's behaviour,
        and what does a ``decisive'' statment even mean?
    These are the questions that statistical methods are meant to address.

    The first step to testing a hypothesis,
        is to know precisely how \textit{likely} any particular outcome is based on that hypothesis.
    The hypothesis for the coin flip is that the probability $p$ of any given flip being heads is 50\%.
    For an experiment consisting of a number of flips $N$,
        the overall probability of seeing an amount of heads $n$ is given by the binomial distribution:
    \begin{equation}
        P(n|p) = \tinymatrix{N\\n} p^n (1-p)^{N-n}
    \end{equation}
    Where $P(n|p)$ is read as ``the probability of seeing $n$ heads \textit{given} their probability $p$.''
    With this, 
    
    basic binomial distribution (coin flip) allows obvious p-test. 
    Using a concrete toy example with numbers,
    Show a binomial PDF distro for N flips, and where on that distro the "data" lies
    Show a C-PDF, and again where the data is,
        and explain that the "unlikeliness" can be used as a test metric, called a "p-value"
    Show of whether or not theory is compatible with data.
    % NOTE: From here on, try to keep the p-value concept,
    %   as well as the PDF and C-PDF, as a central focus.
    % It's easy to conceptualize what a p-value is,
    % so you should keep returning to it in order to retain 
    % a concrete basis for all the weird math you're about to do

    %To begin, let me propose a much simpler experiment than the one described in this analysis.
    %I have a small radioactive source,
    %    which I hypothesis to emit alpha particles a rate of roughly once per minute.
    %How can I test this?
    %This means that, were I to wait for two years,
    %    I would expect a roughly 1 million decays.
    %Significant deviation from this ratio
    %    would indicate my hypothesis was obviously wrong.
    % (e.g.......Alright, so this is why I think I'm going to start with the coin.
    % There's no intuitively "wrong" number of decays in a poisson experiment.
    % By definition, they have a long tail in their distro, which permits values wildly off from the average.
    % I'm pretty sure that in this case you expect more than, like, no particles, and probably less than, idk, a billion?
    % Maybe? The fact that I'm wanting to pull up code to literally calculate this because I'm not sure just proves that this 
    % is a bad example to start with. You *need* the poisson pdf and concept of a p-test to even think about this.
    % FYI, sqrt(1 mil) = 300k, so 5 sigma would be 1.5 mil. So literally no decays wouldn't even qualify as a 5-sigma result,
    % and you would need over 2.5 mil in 2 years to qualify for an overly exceptional one.
    
    Introduce poisson statistics with single counting variable and only signal w/ toy example.
    Discuss difference between coin toss and radioactivity.
    i.e. that with a coin toss, the number to tosses is entirely determined by me, and is thus fixed.
    For decay and such, the number of events is completely random, and what is fixed is how much time (luminosity)
        the experiment is given.

    Introduce background with toy.
    Should also probably dig up the "sensitivity" metric and show how bad that is here as well.
    Ditch toy, pull out actual Background and Signal (SM) event yields.
    The abysmal performance here should justify splitting the event into bins,
        in order to identify regions of Mhh that we are more sensitive to.
    Using multiple bins however, dramatically complicates the statistical analysis.
    Enter the profile likelihood fit.


% Discuss sources of error, assumptions, categorization, concept of mu values;
% basically all the complicated stuff the limit framework is doing
% Make the final goal to be establishing a p-value via the cumulative PDF distro,
%   as was done for the simple coin toss example (to bring things full circle)
\section{VBF \to HH \to 4b Limit-Setting Framework}

    What is a test statistic and why do we need it?

    Description of how "test statistic" q~ is constructed:

    Introduce mu*S+B format.

    Likelihood function

        L = product[ for each category:
            product[for each bin: poissons]
            * product[ nuissance params ] 
        ]


    Discuss how this formula for L specifically functions in this analysis
        (emphasis on explaining the bits in parentheses):

        L = product[ for each category (2: eta hi and lo) 
            product[for each bin: poissons]
            * product[ nuissance params (4 for bgd shape error, 1 for norm error) ] 
        ]
        
    lambda = L(mu, theta vary-opt) / L(mu-opt, theta-opt);
    t = -2*ln(lambda);
    q~ = t with edge cases;

    Single mu vs q~ PDF constructed from monte-carlo distros

    C-PDF showing where mu-model resides for Psb, Pb, and finally Ps=Psb/(1-Pb)
    CLs = CLs+b/CLb explained in \cite{Barlow:2019svl} (pg. 192)

    In practice, the q~ CPDF distros are estimated using an asymptotic approximation method\cite{asymptotic_formulae_for_likelihood}.
    (because the MC method is way too slow)

    

% Finally, I need to reveal what our final results actually are, and what they mean.
% This will mostly just be plots of mu values, limit values, and my 2D exclusion plots.
% I can discuss the shapes and stuff here, as well as talk about any mismatches between expected and observed results.
% Be ready to just put in filler results, since we probably won't have unblinded by the time I get here.
\section{Final Interpretation}

SM mu scan plot of signal p-value.
If you do this using one of the other stats (t, or t~ or something),
    I think you can use it to show that pure discovery of the HH process is impossible right now,
    and therefore justify putting limits on the couplings instead.

multi k2v-scan mu plots

Segway from a coupling of mu plots into the idea that, while the SM point is a lost cause,
    other coupling values can be safely ruled out.
Use this to motivate the idea of performing a "coupling scan"
    which can show which values of a coupling can be ruled out.

1D SM k2v plot
1D SM kl plot?

Multi-kl 1D k2v plots

2D k2v/kl plot

Multi-dimensional slice plots
