\chapter{Trigger System} %TODO
\section{Introduction}
    The amount of data output by the ATLAS detector is immense and overwhelming.
    Processing every single bunch crossing would require completely unphysical bandwidth levels, and would take centuries to process.
    To counteract this overabundance of data, ATLAS relies on its triggering system to drastically reduce its throughput.
    Known simply as the ATLAS trigger system, this critical piece of infrastructure constitutes the last step of data taking, and the first step of physics analysis.

    The trigger system is a series of hardware and software level algorithms designed to quickly identify bunch crossing ``events" which may be interest to physics analysis, while discarding the rest.
    Refered to as ``online" analysis, these algorithms perform event selection ``live" in parallel to the machine running and taking data.
    The trigger system processes all ATLAS events immediately after readout, ultimately reducing the 40 MHz bunch crossing rate to a data output rate of 1 kHz.
    The data which survives this rapid selection is read out to disk and distributed to individual research teams for more sophisticated ``offline" analysis later.
    
    Triggering is achieved by running events through two sequential trigger systems.
    All events first go through the hardware-based Level 1 Trigger (L1) before being being run through the more sophisticated (but slower) software-based High Level Trigger (HLT).
    Both of these triggers involve a plethora of different measurements on various aspects of the events, such as total transverse energy, transverse momentum, jet multiplicity, and opening angles between jets.
    Moreover, there is more than one possible way that an event can pass the trigger process.
    Each of the various kinematic properties the triggers determine have multiple threshold values that can determine a ``pass" (e.g. a jet $p_T$ trigger can have thresholds at 30, 45, or 55 GeV, among others).
    A ``trigger chain" is a combination of several different such kinematic conditions, each with their own thresholds.
    An bunch crossing is ultimately accepted and read out to disk for further analysis offline if it is able to pass all the conditions of a trigger chain.
    As stated earlier, there are several ways an event can pass the triggers, in the sense that there are a huge number of different trigger chains, each permitting a different combinations of permissable kinematics.
    An event is read out if it passes any one of the trigger chains, and is labelled in data with all the trigger chains it passes.
    All of the running trigger chains comprise the ``trigger menu".
    The chains included in the trigger menu were decided upon before the beginning of the Run 2 data taking period, based on input from various analysis teams.


\section{Level 1 Trigger}
    One bunch-crossing every 25 ns is a blistering pace to operate at.
    The pace is so fast, that software-based applications are unable to keep up.
    Instead, the first layer of the trigger is the L1 system, which runs entirely through hardware-level gate logic.
    The goal of this system is to reduce the event rate from the raw 40 MHz bunch-crossing rate, down to a more manaegable rate of 100 kHz \cite{trigger_run2}.
    
    

Input rate of 40 MHz (every 25 ns, the LHC bunch crossing rate), output rate of 100 kHz (utilizes detector buffer memory to keep up).

%Where
As close to ATLAS as possible in order to reduce latency, specifically in the USA15 underground chamber \cite{trigger_tdr}.


%How
Uses only information from calorimeters and Muon Trigger Chamber


Calorimeter Algorithms: \cite{L1_calo_run1}
    Based on Calorimeter information.

    Reduces granularity of cals by clustering sensors together into ``trigger towers" each with a resolution of 0.1x0.1 in $\Delta \eta x \Delta \phi$.

    A ``Region of Interest" (RoI) is created as a group of towers which collectively satisfy various conditions (varying between the trigger in question).

    % TODO condense these three down... or maybe don't? I'll just write it all out and let steve tell me to trim it later
    Cluster Processor Module (CPM)
        Based on Barrel Cals
        For electron/photon and tau/hadron identification.
        Electrons and photons are triggered by clusters which are limited to the ECal and do not penetrate into the HCal.
        Taus and hadrons are identified by the fact that they do penetrate into the HCal.
        Both groups work by checking all possible 4x4 windows of trigger towers, and identifying windows containing an isolated ``Region of Interest" (RoI).
        An RoI is defined as a 2x2 cluster of towers with an Et sum that is a relative maximum compared to surrounding towers.
        This 2x2 RoI is the center of the 4x4 window (see figure [TODO INSERT FIGURE 13 OF L1_calo_run1]).
        Windows are considered as passing the CPM trigger if the RoI satisfies an isolation requirement, meaning that the 12 towers surrounding that core fall \textit{below} a predifined Et ``isolation threshold" value.

    Jet/Energy Module (JEM)
        Uses Barrel and Endcap, and does not distinguish between ECal and HCal
        Basic units of data collection are 2x2 collections of trigger towers called ``jet elements", resulting in minimum resolution of 0.2x0.2 in $\Delta \eta x \Delta \phi$.
        Windows can vary in size, but must be based around a core of 2x2 jet element RoI which are (as with the CPM) a local maximum in Et.
        Counts multiplicity of hits in each region and begins Et sums for the MET calculation.
        
    Common Merger Module (CMM) -> replaced by Cluster Merger Modules (CMX) in Run 2 \cite{trigger_run2}
        Uses all Cals including FCal.
        Carries out final jet multiplicity counting and Et/missing Et sums

L1 Muon Trigger \cite{trigger_run1}
    Uses the Muon Trigger Chamber %\ref{sec:muon-trigger_chamber} FIXME uncomment 
    to trigger based on particle pt and multiplicity


%\section{Level 2 Trigger} Software level trigger on L1-based ROIs. Merged with HLT in Run 2

\section{High Level Trigger}

%What
Software trigger using data from all detector elements (including the Inner Detector).

%Why
To further reduce event levels to levels permitting full offline analysis.

%When
Input rate of 100 kHz (output of L1), output rate of ~1 kHz.

%Where
Also very close to ATLAS, at the surface, in building SCX1. %see trigger_tdr figure 2

%How
The HLT performs rapid online reconstruction of event data [FIXME forward reference reconstruction here] into various physics ``signatures", with each signature corresponding to specific particles or physics processes.
The main signatures used in ATLAS are: minimum bias signatures, electron/photons (Egamma), muons, jets, taus, missing transverse energy (MET), b-jets (as in jets from bottom quarks), and B-physics (as in B-hadrons).
The process of reconstruction and the algorithms applied to these reconstructed signatures are based on offline software algorithms.
These offline algorithms are adjusted for use in the online environment, which is discussed further in the next chapter.
    


% TODO: I think I'm going to move this into the "event reconstruction" chapter, since discussion of the retuning process is entirely based on discussion of the individual low-level and high-level algorithms used for the bjet trigger
%\section{HLT Retuning Framework}
%Finally something that I did!
%Except it didn't end up mattering...
%Do I need to mention specifically my work on the FTK? NOPE. Just talk about the retuning at a more general level.
