\chapter{Trigger System}

\section{Introduction}
    The amount of data output by the ATLAS detector is immense and overwhelming.
    Processing every single bunch crossing would require completely unphysical bandwidth levels, and would take centuries to process.
    To counteract this overabundance of data, ATLAS relies on its triggering system to drastically reduce its throughput.
    Known simply as the ATLAS trigger system, this critical piece of infrastructure constitutes the last step of data taking, and the first step of physics analysis.

    The trigger system is a series of hardware and software level algorithms designed to quickly identify bunch crossing ``events" which may be interest to physics analysis, while discarding the rest.
    Refered to as ``online" analysis, these algorithms perform event selection ``live" in parallel to the machine running and taking data.
    The trigger system processes all ATLAS events immediately after readout, ultimately reducing the 40 MHz bunch crossing rate to a data output rate of 1 kHz.
    The data which survives this rapid selection is read out to disk and distributed to individual research teams for more sophisticated ``offline" analysis later.
    
    Triggering is achieved by running events through two sequential trigger systems.
    All events first go through the hardware-based Level 1 Trigger (L1) before being being run through the more sophisticated (but slower) software-based High Level Trigger (HLT).
    Both of these triggers involve a plethora of different measurements on various aspects of the events, such as total transverse energy, transverse momentum, jet multiplicity, and opening angles between jets.
    Moreover, there is more than one possible way that an event can pass the trigger process.
    Each of the various kinematic properties the triggers determine have multiple threshold values that can determine a ``pass" (e.g. a jet $p_T$ trigger can have thresholds at 30, 45, or 55 GeV, among others).
    A ``trigger chain" is a combination of several different such kinematic conditions, each with their own thresholds.
    An bunch crossing is ultimately accepted and read out to disk for further analysis offline if it is able to pass all the conditions of a trigger chain.
    As stated earlier, there are several ways an event can pass the triggers, in the sense that there are a huge number of different trigger chains, each permitting a different combinations of permissable kinematics.
    An event is read out if it passes any one of the trigger chains, and is labelled in data with all the trigger chains it passes.
    All of the running trigger chains comprise the ``trigger menu".
    The chains included in the trigger menu were decided upon before the beginning of the Run 2 data taking period, based on input from various analysis teams.


\section{Level 1 Trigger}
    One bunch-crossing every 25 ns is a blistering pace to operate at.
    The first layer of the trigger system, L1, thus runs entirely through hardware-level gate logic.
    The goal of this system is to reduce the event rate from the raw 40 MHz bunch-crossing rate, down to a more manaegable rate of 100 kHz \cite{trigger_run2}.
    To reduce latency as much as possible, all the electronics comprising L1 are located as close as possible to ATLAS itself, specifically in the USA15 underground chamber \cite{trigger_tdr}.
    As yet another consequence of the high frequency L1 must operate at, it exclusively uses information from the ATLAS calorimeters and Muon Trigger Chamber for its decisions.
    %(utilizes detector buffer memory to keep up).

    The Muon Trigger aspect of L1 is based entirely around the trigger-dedicated Muon Trigger Chamber, which described in section. %\ref{sec:muon-trigger_chamber} FIXME uncomment 
    Its purpose is to make trigger decisions primarily based on muon $p_T$ and track multiplicity in the Trigger Chamber  \cite{trigger_run1}.
    In the calorimeter-based trigger, the selection algorithm involves the construction of key trigger objects that are crucial to understanding the events used in ATLAS analysis.

    The full granularity of the ATLAS calorimeters is too complex to analyze in the \textasciitilde 25 ns L1 has to process each bunch-crossing.
    Instead, the various sensors of the calorimeters are clustered together into ``trigger towers", each with a resolution of 0.1x0.1 in $\Delta \eta x \Delta \phi$.
    A ``Region of Interest" (RoI) is created as a group of towers which collectively satisfy the condition of a specific type of trigger.
    The way RoIs are formed and used varies between the different L1 calorimeter trigger modules: the CPM, JEM, and CMX.

    The Cluster Processor Module (CPM) exclusively uses the Barrel Calorimeters to function, and is primarily meant for rapid identification of electrons/photons and taus/hadrons.
    In either situation, the CPM's first step is to check all possible 4x4 ``windows" of trigger towers, identifying windows containing an isolated ``Region of Interest" (RoI).
    Here, an RoI is defined as a 2x2 cluster of towers with an $E_T$ sum that is a relative maximum compared to surrounding towers.
    This 2x2 RoI is the center of the 4x4 window (see figure [TODO INSERT FIGURE 13 OF L1_calo_run1]).
    Windows are considered as passing the CPM trigger if the RoI satisfies an isolation requirement, meaning that the 12 towers surrounding that core fall \textit{below} a predifined $E_T$ ``isolation threshold" value.
    Electrons and photons are then seperated from taus and hadrons by the fact that the latter group penetrates into the HCal barrel, while the former group stays highly contained to the ECal.

    Expanding out, the Jet/Energy Processing Module (JEM, or sometimes JEP) makes use of the calorimeter barrels and endcaps, as well as the FCAL, though it does not distinguish between the ECal and HCal.
    The JEM further reduces the granularity under consideration, with a basic unit of data collection being 2x2 collections of trigger towers called ``jet elements", resulting in minimum resolution of 0.2x0.2 in $\Delta \eta x \Delta \phi$.
    Like the CPM, the JEM runs its trigger conditions on windows of multiple jet elements that must be based around a 2x2 RoI core (which is a local $E_T$ maximum).
    Unlike the CPM, these windows can vary in size.
    Primarily, the JEM is intended to perform hit multiplicity counting as well as assist the Extended Cluster Merger Modules (CMX) in carrying out the final jet multiplicity and $E_T$ sums \cite{L1_calo_run1}\cite{trigger_run2}.


\section{High Level Trigger}
    After the L1 Trigger has reduced the event rate to 100 kHz, the software-based High Level Trigger (HLT) is used to further reduce the event rate to the final output of \textasciitilde 1 kHz.
    Located in the SCX1 building (TODO insert trigger_tdr figure 2) at the surface of P1 (again to minimize latency), the HLT uses data from all detector elements completely reconstruct the event as it occured in ATLAS, and performs its selections based on this reconstructed event.
    Different physics processes and particles, known as physics ``signatures", are reconstructed in different ways, and have different triggers based around them. 
    The main signatures used in ATLAS are: minimum bias signatures, electron/photons (Egamma), muons, jets, taus, missing transverse energy (MET), b-jets (as in jets from bottom quarks), and B-physics (as in B-hadrons).
    The process of reconstruction and the trigger algorithms applied to these reconstructed signatures are based on offline software algorithms.
    These offline algorithms are adjusted for use in the online environment, which is discussed further chapter [TODO].
    Once events have successfully passed both L1 and the HLT, they are finally distributed offsite for analysis by different physics groups.
    
    

% TODO: I think I'm going to move this into the "event reconstruction" chapter, since discussion of the retuning process is entirely based on discussion of the individual low-level and high-level algorithms used for the bjet trigger
%\section{HLT Retuning Framework}
%Finally something that I did!
%Except it didn't end up mattering...
%Do I need to mention specifically my work on the FTK? NOPE. Just talk about the retuning at a more general level.
