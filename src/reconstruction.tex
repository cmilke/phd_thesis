\chapter{Event Reconstruction}

%    Oh god how do I even start with this crap. These two are so tightly linked and there's no obvious way to speparate them AHHHHHH.
    
%    Once a bunch crossing event has cleared the Level 1 Trigger system, the process of event reconstruction begins.
%    An event in ATLAS is, initially, nothing but a collection of electrical signals emitted from the various detectors.
%    Event reconstruction is the process wherein detector readings are aggregated together into meaningful patterns,
%        which are interpreted as physical objects and processes.
%


    Ok let's just get the basic objects contructed first, since nothing else (triggers, later reconstruction, final selection)
        make sense until I have them.

    This entails ... what? For VBF 4b I think we have:


    %    I should have an image for this in some way %TODO
    \section{Tracks}
        Intro about tracks being the first things we see because they're assembled from the inner detector or something.
        Also I should probably explain what a track is...
        Might need to explain helix parameters as well

        %Clusterization
        The first step to producing a track is the process of clusterization.
        Ionizing particles often deposit energy across several adjacent pixels on a given layer.
        A \textit{connected component analysis} algorithm is used to group pixels together. 
        Based on the pattern of energy distribution in these groups,
            a \textit{space-point} is created indicating the estimated position at which a particle crossed the detector.
        Several space-points can be assigned to the same pixel cluster,
            if the energy deposition pattern suggests multiple particles traversed the same location.

        %Combinatorial track finding
        Initial guesses at tracks, called track seeds, are formed by assembling all realistic combinations of three space-points.
        The track seeds are assigned helix parameters by assuming they travel through a uniform magnetic field,
            allowing immediate estimates of the tracks' momentum.
        These seeds are expanded into \textit{track candidates},
            by including more space-points across additional detectors in the ID using a Kalman filter.

        %Ambiguity solving; NN clustering; Track fit
        A number of criteria are then used to reject poor-quality tracks, as well as to assign scores to all remaining track candidates.
        The scores are then used to resolve ambiguities where multiple tracks are assigned to the same space-points,
            with preference given to higher-scoring candidates.
        Neural networks are used to assist in some ambiguity solving situations,
            as well as to help identify clusters with multiple valid tracks.
        Once ambiguities have been resolved and all malformed track candidates removed,
            the remaining tracks are refit using all available information at high-resolution.
        \cite{atlas_track_reco_performance}

    \section{Jets}
        Jets are an algorithm.
        Specifically, the anti-$k_t$ algorithm, with $\Delta R = 0.4$.
        Something about topological-clusters (topo-clusters).
        \cite{anti_kt}
        Something about assuming hard-scatter jets come from the primary vertex.
        Also explain what a primary vertex is.
        FYI, a primary vertex is a "reconstructed vertex with at least two associated tracks, and the largest sum of squared track momentum".

        Jets created using only calorimeter-based topological-clusters (at the electromagnetic scale [what does this mean??]) %TODO?
            are reffered to as \textit{EMtopo Jets}.
        The jets for this analysis use a more advanced jet algorithm, called \textit{Particle Flow},
            which are constructed by matching tracks from the inner detector to topo-clusters from the calorimeter,
            based on both location and energy projections.
        The resulting \textit{PFlow Jets} are then used forthwith.
        \cite{pflow}
        \cite{jet_energy_scale13TeV}

    \section{Flavour Tagging}
        Of crucial importance:
            b-jets used for higgs, (I should justify this with the high branching ratio of H->b,bbar);
            VBF initial scatter jets anti-b-tagged

        Flavour Tagging is performed in two-stages:
            first with basic low-level taggers,
            which are then used as inputs for the high level taggers.

        \subsection{Low Level Taggers}

            IPxD - IP2D and IP3D - Impact Parameter 2/3 Dimensional; 
            Vertex Algos - SV1 (Secondary Vertex 1) and JetFitter

        \subsection{High Level Taggers}

            MV2, a Boosted Decision Tree (BDT);
            DL1, a Deep Learning Neural Network (DL1r is what we specifically use)
        \cite{bjet_id_and_performance}
        \cite{btagging_optimisation}

    \section{Online VS Offline Reconstruction}
        I should explain how online and offline reco differs;
            it seems to just come down to the fact that the HLT doesn't neccesarily use the most high resolution calorimeter information,
            and often generates tracks using a faster, less acurate algorithm.
            
        I should also describe the mv2 reweighting procedure here
            (We don't use mv2 for the late-stage b-tag requirement on the higgs decay candidates,
            but we DO use it in the triggers):
            Step1.1 - Run HLT reco on MC files;
            Step1.2 - Return low level taggers on MC HLT reco;
            Step2   - Rerun HLT reco on MC, now with retuned low level tagging info;
            Step3.1 - Perform BDT training for MV2 on HLT MC reco w/ low level tags;
            Step3.2 - Convert training output to useable histograms;
            Step4   - Rerun HLT reco on MC once more, to get full retuned tagging ouput. Use output performance to determine working points;


    \section{Scale Factors}
        Oh this one's going to suck...


    \section{Other?}
        ... is that it? I'll look around to see if there's anything else.



    %Reconstruction is performed for an event twice.
    %It is first done rapidly, using coarse measurments and calculations,
    %    for the purpose of allowing the aforementioned HLT to trigger events for readout.
    %These events which pass the trigger are passed off to a massive global computer cluster reffered to as the GRID.
    %No longer bound by the stringent time contrainsts of the ``online'' running environment,
    %    the GRID is able to devote vast amounts of time and computing power to a second, 
    %    more comprehensive, reconstruction of each event.
    %For both the ``online'' (HLT) and ``offline'' (GRID) environments, event reconstruction is carried out by the same suite of software,
    %    called \textit{Athena}.
